\documentclass[]{article}

\usepackage{nameref}
\usepackage[a4paper,top=3cm,bottom=4cm,left=3.5cm,right=3.5cm]{geometry}
\usepackage[font=footnotesize,labelfont={sf,bf}]{caption}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}
\usepackage{lmodern}
\usepackage{diagbox}
\usepackage{booktabs}
\usepackage{siunitx}

\graphicspath{ {./assets/latexImages/} }

\def\code#1{\texttt{#1}}

\title{SPM Project: BSP}
\author{Lorenzo Bellomo, 531423}
\date{AA 2018/2019}

\begin{document}

\maketitle

\section{Introduction}

In this report the \emph{Bulk Synchronous Pattern} (BSP) will be analysed both from a theoretical viewpoint and from a practical one. \\
The outline of the report will be the following:
\begin{itemize}
	\item Section \ref{sec:parDesign} (\nameref{sec:parDesign}).
	\item Section \ref{sec:perfModel} (\nameref{sec:perfModel}).
	\item Section \ref{sec:implDetails} (\nameref{sec:implDetails}).
	\item Section \ref{sec:eval} (\nameref{sec:eval}).
	\item Section \ref{sec:conclusion} (\nameref{sec:conclusion}).
\end{itemize}

\section{Parallel Architecture Design}
\label{sec:parDesign}

BSP is by nature a parallel bridging model. The main idea is that the process is divided in three phases (as shown in figure \ref{fig:BSP}):
\begin{itemize}
	\item \emph{Local computation}: The processors act independently and compute their task on a different partition of the input data.
	\item \emph{Communication}: In this phase processors are allowed to send data to the other ones. It is important to notice that phase 1 and 2 (super step and communication) may overlap in case of uneven input workload.
	\item \emph{Synchronization}: In this phase all the processors synchronize and wait that the communication phase is over for each worker (\emph{barrier}).
\end{itemize}
This whole process made of 3 phases composes a \emph{super step}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\linewidth]{bsp}
	\caption{BSP structure}
	\label{fig:BSP}
\end{figure}

\section{Performance Modelling}
\label{sec:perfModel}

In order to theoretically model the performances of a BSP algorithm some values are going to be defined:
\begin{itemize}
	\item $\mathit{nw}$ is the number of workers (BSP processors).
	\item $k$ is the number of super steps
	\item $w_i$ is the time spent by the \emph{i-th} worker in the compute phase.
	\item $m_i$ is the number of maximum messages receiver by the \emph{i-th} worker.
	\item $c$ is the cost of sending or receiving one message.
	\item $B$ is the cost of the synchronization step (\emph{barrier}).
	\item $I$ and $F$ are relatively the initialization and finalization cost. Their value is application specific.
\end{itemize}
The cost of a super step ($\mathit{SS}$) can be approximated (upper bounded) as follows:
$$ \mathit{SS}_j = \max_{i=1}^{\mathit{nw}} w_i + \max_{i=1}^{\mathit{nw}} c m_i + B $$
Which eventually gives:
$$ \mathit{tot\_cost} = I + \sum_{j=1}^{k} \mathit{SS}_j + F $$
\subsection{Opportunities for parallelism}
According to the model provided, the opportunities for parallelization reside in the \emph{super step cost}, which ideally should decrease linearly by increasing the number of parallel activities. In particular, it is safe to assume that $w_i = \Theta(n/\mathit{nw}) \, \forall i$, while instead both the communication cost and the barrier cost are pure overhead, which grows with $\mathit{nw}$. \\
In this particular problem, the parallel design is already provided, but the goal of the implementation is to minimize the overheads, with particular focus on the \emph{communication} one, which can be assumed to be the most costly. \\

For what regards the user provided business logic code, which regards both the computation phase and the communication one, the scalability of the provided \emph{BSP} depends on the goodness of the business logic code.

\section{Implementation Structure and Details}
\label{sec:implDetails}

All the source code files are under directory \emph{src} and are provided under the form of header only files. \\The files provided are:
\begin{itemize}
	\item \code{sorter.cpp}: it contains the main method. 
	\item \code{sorterLogic.hpp}: it contains the business logic code for the custom \emph{BSP} execution with standard \emph{POSIX} threads.
	\item \code{posixBSP.hpp}: it contains the implementation (with \emph{POSIX} threads) of the BSP pattern.
	\item \code{logicBSP.hpp}: it contains the interface that hosts the business logic code abstraction. Every application should subclass the one provided in this file to provide the business logic code. More in section \ref{sec:business}.
	\item \code{barrier.hpp}: it contains an implementation of a reusable (context aware) barrier, implemented with a mutex and a condition variable.
	\item \code{safeQueue.hpp}: it contains the queue implementation used. Its discussion will be delayed to section \ref{sec:queue}.
	\item \code{queueMatrix.hpp}: It contains a class to handle a matrix of queues. Its use will be to give to every worker a queue \emph{for each} super step. The matrix dimension will be $matrix[\#\mathit{ss}][\mathit{nw}]$.
	\item \code{utimer.hpp}
	\item \code{makefile}
	\item \code{benchmark}: This folder will be analysed in section \ref{sec:benchFold}.
\end{itemize}

\subsection{Communications}
\label{sec:queue}
The \emph{queue} implementation is a modification of the one provided during classes. The main difference is that \emph{writes} to the queue are synchronized, while \emph{reads} are not. \\ This is made possible by the fact that reads are always performed after a barrier, which makes the synchronization implicit. Writes have to be executed in \emph{mutual exclusion} because multiple producers may potentially write together. In order to reduce the synchronization overhead, it is suggested to use the method \code{push\_multiple(iterator, iterator)}, in order to lock once the queue and then release it immediately.

\subsection{Threads}
The code of the threads is a private function (\code{worker}) in file \code{posixBSP.hpp}. \\
As explained, one thread executes the code of one worker (which may comprise more super steps), so in total only $\mathit{nw}$ threads are spawned by the runtime.


\subsection{Business Logic Code}
\label{sec:business}
In order to write code that can be fed to the provided BSP implementation, one has to subclass \code{logicBSP}. This is an interface that models a generic worker (see \code{sorterLogic.hpp} for an example). \\
One has to provide the code of the worker's \emph{super steps} (every worker has its own local state) and it must provide a switcher function that maps every function to a super step number. \\
The code of a generic super step has to be a function with this signature (except for the function name, which might vary): 
\begin{figure}[H]
	\centering
	\begin{minipage}{0.9\textwidth}		
		\code{void ss(logicBSP::ss\_queue, size\_t, std::vector<logicBSP::ss\_queue>)}
	\end{minipage}
\end{figure}
Where \code{logicBSP::ss\_queue} is just an alias for a pointer to a \code{safe\_queue<T>}. The runtime will give the handle to the workers' \emph{input queue} and the collection of \emph{output\_queues}, indexed exactly like the workers (where \code{output\_queues[i]} is the queue belonging to worker \code{i}). \\
This way, every worker keeps its own local state, which survives every super step call.

\section{Experimental Evaluation}
\label{sec:eval}
Benchmark data were collected with the provided scripts in the \code{benchmark} folder (see section \ref{sec:benchFold}), while the data were gathered and displayed in section \ref{sec:plots}. \\
The test have been made on the \emph{Xeon Phi} and the results shown refer exclusively to data collected on that machine. \\
The project was compiled with \code{g++} version \code{7.4.0} provided on the machine, and with the options provided in the \code{makefile}. \\
The program can be compiled with different preprocessor \code{define} in order to change a bit the behaviour of the programs. The possibilities are listed below (and are used in the \code{makefile} for various rules):
\begin{itemize}
	\item \code{DEBUG}: (used in rule \code{make debug}) it activates the flag \code{-g} useful for debugging with \code{gdb}, and it checks that the output vector is equal to the one computed using \code{std::sort}.
	\item \code{BENCHMARK}: (used in rule \code{make benchmark}) it skips computing the \code{std::sort}, and only sorts the input array via the \emph{Tiskin} algorithm.
	\item \code{TSEQ}: (used in rule \code{make tseq}) this signals that the user wants to collect some partial execution data (like the synchronization time, the super step time and so on). In this case the final time is higher due to the collection of partial times overhead.
\end{itemize}

\subsection{Benchmark Folder}
\label{sec:benchFold}
This folder is organized as follows:
\begin{itemize}
	\item \code{benchmark.sh}: This bash script automates the benchmarking process. It starts the program with different parallelism degrees and different input vector sizes. It executes every combination of $n$ (size of input vector) and $\mathit{nw}$ 3 times in order to then compute an average of each time. The analysis of its output is delayed to section \ref{sec:benchmark}.
	\item \code{gprof-helper.c}: This file is needed in order to allow \code{gprof} profiling for multithreaded applications. It provides wrappers for the \code{pthread} creation.
	\item \code{profiler.sh}: This bash script automates the profiling process. At the end of the script execution, it dumps the profiling output in a file, by starting the program with various parallelism degrees. The analysis of its output is delayed to section \ref{sec:profile}.
	\item \code{tseq.sh}: This bash script collects some times from the program execution. In particular, it collects, for each worker, the \emph{super step} time, the \emph{barrier} synchronization delay and something similar. Also in this case, times are collected with various parallelism degrees. The analysis of its output is delayed to section \ref{sec:tseq}.
\end{itemize}
All the cited scripts collect data and dump them in various files, that go in the folder \code{benchmark/data/}, which gets generated by those scripts.

\subsection{Collection of Partial Times}
\label{sec:tseq}
In table \ref{tab:tseq} some partial times collected are shown. All the times are expressed in microseconds and are the result of an average between multiple runs. All the runs have been executed with $3\,000\,000$ items ($2^{20} < 3M < 2^{21}$) as an input size.\\ 
The labels shown are described below:
\begin{itemize}
	\item \emph{ss computation}: it is an average of the time spent by a generic super step (average of the three) in computation phase.
	\item \emph{ss communication}: same thing but related to communications.
	\item \emph{barrier sync}: time spent by a generic super step waiting for all the workers to end their local computation and their communication. This is the value with the highest \emph{variance}.
	\item \emph{end process}: time spent by a generic worker waiting for the final synchronization, needed to assert the validity of the global continuation clause.
	\item \emph{whole ss}: time spent by a generic worker in a generic super step (both communication and computation phase, but no synchronization).
\end{itemize}
\begin{table}[H]
	\centering
	\begin{tabular}{l|*{9}r}
		\toprule
		\diagbox{phase}{$\mathit{nw}$} 
		& 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128 & 256 \\
		\midrule
		barrier sync & 9 & 54 & 25 & 18 & 10 & 8 & 10 & 15 & 95 \\
		communication & 0 & 11 & 10 & 7 & 5 & 7 & 17 & 7 & 216 \\
		computation & 300 & 243 & 86 & 44 & 24 & 15 & 17 & 42 & 167 \\
		whole ss & 293 & 250 & 93 & 49 & 28 & 20 & 29 & 47 & 316 \\
		end process & 0 & 0 & 1 & 1 & 1 & 3 & 7 & 16 & 46 \\
		\bottomrule
	\end{tabular}%
	\caption{Collections of partial times (in \SI{}{\milli \second})}
	\label{tab:tseq}%
\end{table}%

\subsection{Benchmark}
\label{sec:benchmark}
The raw data collected through the \code{benchmark.sh} script are shown in table \ref{tab:benchmark}. The time is shown in milliseconds, and it is the result of an average of multiple runs. The last column refers to the times collected using \code{std::sort}.
\begin{table}[H]
	\centering
	\begin{tabular}{l|*{10}r}
		\toprule
		\diagbox{$n$}{$\mathit{nw}$} 
		& 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128 & 256 & \code{std::sort} \\
		\midrule
		$2^{20}$ & 294 & 306 & 146 & 86 & 64 & 66 & 113 & 336 & 2\,684 & 186 \\
		$2^{21}$ & 595 & 645 & 258 & 161 & 112 & 96 & 144 & 343  & 2\,675 & 392 \\
		$2^{22}$ & 1\,219 & 1\,417 & 530 & 303 & 203 & 152 & 200  & 381 & 2\,643 & 823 \\
		$2^{23}$ & 2\,494 & 2\,787 & 1\,060 & 586 & 362 & 280 & 295 & 722 & 2\,211  & 1\,705 \\
		$2^{24}$ & 5\,175 & 5\,781 & 2\,431 & 1\,186 & 712 & 454 & 429 & 923 & 1\,539 & 3\,577 \\
		$2^{25}$ & 10\,751 & 12\,309 & 4\,867 & 2\,575 & 1\,393 & 851 & 770 & 1\,198 & 1\,797 & 7\,460 \\
		$2^{26}$ & 22\,390 & 25\,220 & 11\,353 & 5\,191& 3\,245 & 1\,665 & 1\,224 & 1\,543 & 4\,016 & 15\,579 \\
		$2^{27}$ & 48\,066 & 56\,732 & 23\,548 & 12\,145 & 6\,788 & 4\,928 & 2\,191 & 2\,132 & 5\,041 & 30\,804 \\
		
		\bottomrule
	\end{tabular}%
	\caption{Time collected and averaged (in \SI{}{\milli \second})}
	\label{tab:benchmark}
\end{table}%
From a quick view, it is possible to notice that for a parallelism degree of 1 and 2, the running time is comparable. This seems to be due to the fact (according to the data collected with script \code{tseq.sh}) that, while super step 1 correctly "halves" its completion time on average, the third one increases a lot in running time. This is normal since super step 3 with 1 worker doesn't require queues and message passing, while the same cannot be said for 2 workers. \\
In addition to this, it is also possible to notice that (as it should be safe to assume), increasing the input size, the program scales better. \\
The program seems to scale well up until nw = 32 (with the exception of nw = 2). \\
When nw = 64, the program scales only for bigger input sizes (and by extension it is safe to assume that the same behaviour applies to nw = 128). For nw = 256 instead the overhead is huge. \\
Some plots related to those data are shown in section \ref{sec:plots}.


\subsection{Profiling}
\label{sec:profile}
According to \code{gprof}, the most time is spent iterating between elements of a vector. This is also due to the fact that a lot of time is spent in \emph{generating} the input vector, and as the size grows of the input grows this time gets longer and longer. \\
That said, the most time consuming operations used by the workers, excluding the time spent iterating between elements, which is hard to monitor, are the \emph{queue} related ones.
Around 2\% of the time is spent either \emph{pushing} or \emph{popping} from queues. \\
The longest super step is super step 2, which accounts for more than double the time spent in super steps 1 and 3.\\
The time spent locking and synchronizing around a barrier instead results \emph{negligible}. \\
In order to execute again this evaluation, use the script \code{profile.sh}.

\subsection{Plots}
\label{sec:plots}
In this section some plots related to data generated with the script \code{benchmark.sh} are shown.\\
Data are usually shown for the highest input size ($2^{27}$) and for an average on the used input sizes (for each $ 2^i, i \in [20,27]$).

\subsubsection{Completion Time}
In figure \ref{fig:execution} the completion time is shown on the left for an input size of $2^{27}$, while on the right the average is shown. The crossing line is the time spent with \code{std::sort}. 
\begin{figure}[H]
	\subfloat{{\includegraphics[width=0.50\linewidth]{execution1} }}
	\qquad
	\subfloat{{\includegraphics[width=0.50\linewidth]{execution2} }}%
	\caption{Completion time with max input size vs the average. (in \SI{}{\micro \second})}
	\label{fig:execution}
\end{figure}

\subsubsection{Speedup}
The \emph{speedup} is computed as $s(p) = T_{seq}/T_{par}(p)$. The best sequential time $T_{seq}$ taken as reference is the one obtained using \code{std::sort}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{speedup}
	\caption{Speedup ($s(p) = T_{seq}/T_{par}(p)$)}
	\label{fig:speedup}
\end{figure}

\subsubsection{Scalability}
The \emph{scalability} is computed as $\mathit{scalab}(p) = T_{par}(1)/T_{par}(p)$
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{scalability}
	\caption{Scalability ($\mathit{scalab}(p) = T_{par}(1)/T_{par}(p)$)}
	\label{fig:scalab}
\end{figure}

\subsubsection{Efficiency}
The \emph{efficiency} is computed as $\epsilon(p) = T_{seq}/(pT_{par}(p))$
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{efficiency}
	\caption{Efficiency ($\epsilon(p) = T_{seq}/(pT_{par}(p))$)}
	\label{fig:effic}
\end{figure}

\section{Conclusions}
\label{sec:conclusion}

\end{document}
